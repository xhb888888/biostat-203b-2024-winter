---
title: "Biostat 203B Homework 5"
subtitle: Due Mar 22 @ 11:59PM
author: "Hanbei Xiong 605257780"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
---

```{r}
library(GGally)
library(gtsummary)
library(tidyverse)
library(tidymodels)
library(ranger)
library(stacks)
library(kernlab)
library(xgboost)
library(vip)
```

## Predicting ICU duration

Using the ICU cohort `mimiciv_icu_cohort.rds` you built in Homework 4, develop at least three machine learning approaches (logistic regression with enet regularization, random forest, boosting, SVM, MLP, etc) plus a model stacking approach for predicting whether a patient's ICU stay will be longer than 2 days. You should use the `los_long` variable as the outcome. You algorithms can use patient demographic information (gender, age at ICU `intime`, marital status, race), ICU admission information (first care unit), the last lab measurements before the ICU stay, and first vital measurements during ICU stay as features. You are welcome to use any feature engineering techniques you think are appropriate; but make sure to not use features that are not available at an ICU stay's `intime`. For instance, `last_careunit` cannot be used in your algorithms. 

1. Data preprocessing and feature engineering.

**Answer:**

We remove features that will not be used in this task. We also convert `los_long` to a factor variable so it can be used as the outcome variable in the model. 

```{r}
mimiciv_icu_cohort <- readRDS("./mimic_icu_cohort.rds")
```

```{r}
# Remove features that will not be used in this task
mimiciv_icu_cohort <- mimiciv_icu_cohort |>
  arrange(subject_id, hadm_id, stay_id) |>
  select(-c(subject_id, hadm_id, stay_id, last_careunit, intime, outtime, 
            admittime, dischtime, deathtime, admit_provider_id, 
            discharge_location, language, edregtime, edouttime, anchor_age,
            anchor_year, anchor_year_group, dod, hospital_expire_flag, 
            admission_type, admission_location, insurance, los))
```

```{r}
mimiciv_icu_cohort <- mimiciv_icu_cohort |>
  mutate(los_long = ifelse(los_long == TRUE, 1, 0)) |>
  mutate(los_long = as.factor(los_long))
```


2. Partition data into 50% training set and 50% test set. Stratify partitioning according to `los_long`. For grading purpose, sort the data by `subject_id`, `hadm_id`, and `stay_id` and use the seed `203` for the initial data split. Below is the sample code.

**Answer:**

Data is split into 50% training set and 50% test set. The split is stratified according to `los_long`. The data has been sorted in the previous question.

```{r}
#| eval: True
set.seed(203)

data_split <- initial_split(
  mimiciv_icu_cohort, 
  # stratify by los_long
  strata = "los_long", 
  prop = 0.5
  )
```

```{r}
data_split
```

```{r}
other <- training(data_split)
dim(other)
test <- testing(data_split)
dim(test)
```

3. Train and tune the models using the training set.

**Answer:** Three independent machine learning algorithm and one stacked model based on these three algorithms were tested in this section. Different parameter tuning methods were used because of the limitation of computation. All tuning were done on training set with 3 folds cross validation. The best set of parameters were selected based on the AUC score of the model. The entire workflow with the best set of parameters was applied to the test set to evaluate the performance of the model. Feature importances were also reported at the end of each model section. 

# Logistic Regression

```{r}
# Impute the missing values and normalize the data
logit_recipe <- 
  recipe(
    los_long ~ ., 
    data = other
  ) |>
  step_impute_mean(Sodium) |>
  step_impute_mean(Glucose) |>
  step_impute_mean(Chloride) |>
  step_impute_mean(Potassium) |>
  step_impute_mean(Creatinine) |>
  step_impute_mean(Hematocrit) |>
  step_impute_mean(Bicarbonate) |>
  step_impute_mean(White_Blood_Cells) |>
  step_impute_mean(Heart_Rate) |>
  step_impute_mean(Respiratory_Rate) |>
  step_impute_mean(Temperature_Fahrenheit) |>
  step_impute_mean(Non_Invasive_Blood_Pressure_systolic) |>
  step_impute_mean(Non_Invasive_Blood_Pressure_diastolic) |>
  step_impute_mode(marital_status) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_numeric_predictors()) |> 
  step_normalize(all_numeric_predictors()) |>
  print()
```

```{r}
# Define parameters to tune in later steps
logit_mod <- 
  logistic_reg(
    penalty = tune(), 
    mixture = tune()
  ) |> 
  set_engine("glmnet", standardize = FALSE) |>
  print()
```

```{r}
logit_wf <- workflow() |>
  add_recipe(logit_recipe) |>
  add_model(logit_mod) |>
  print()
```

```{r}
# Exhaustive grid search
param_grid <- grid_regular(
  penalty(range = c(-6, 3)), 
  mixture(),
  levels = c(100, 5)
  )
```


```{r}
set.seed(203)

folds <- vfold_cv(other, v = 3)
```


```{r}
# tuning using the grip seach
(logit_fit <- logit_wf |>
  tune_grid(
    resamples = folds,
    grid = param_grid,
    control = control_grid(verbose = TRUE, save_pred = FALSE),
    metrics = metric_set(roc_auc, accuracy)
    )) |>
  system.time()
```

```{r}
logit_fit |>
  # aggregate metrics from 3 folds
  collect_metrics() |>
  print(width = Inf) |>
  filter(.metric == "roc_auc") |>
  ggplot(mapping = aes(x = penalty, y = mean, color = factor(mixture))) +
  geom_point() +
  labs(x = "Penalty", y = "CV AUC") +
  scale_x_log10()
```
```{r}
logit_fit |>
  show_best("roc_auc")
```
```{r}
best_logit <- logit_fit |>
  select_best("roc_auc")
```

```{r}
final_wf_logit <- logit_wf |>
  finalize_workflow(best_logit)
```

```{r}
final_fit_logit <- 
  final_wf_logit |>
  last_fit(data_split)

final_fit_logit |> 
  collect_metrics()
```

```{r}
final_model <- extract_fit_parsnip(final_fit_logit)
coefficients <- tidy(final_model)
print(coefficients)
```
```{r}
vip(final_model, num_features = 20, method = "model")
```

**Summary:** The AUC is 0.5964 and the accuracy is 0.5754 on the test set. The most important features with relative similar importance are 'first care units' and 'heart rate'.

# Random Forest

```{r}
# Impute the missing values and normalize the data
rf_recipe <- 
  recipe(
    los_long ~ ., 
    data = other
  ) |>
  step_impute_mean(Sodium) |>
  step_impute_mean(Glucose) |>
  step_impute_mean(Chloride) |>
  step_impute_mean(Potassium) |>
  step_impute_mean(Creatinine) |>
  step_impute_mean(Hematocrit) |>
  step_impute_mean(Bicarbonate) |>
  step_impute_mean(White_Blood_Cells) |>
  step_impute_mean(Heart_Rate) |>
  step_impute_mean(Respiratory_Rate) |>
  step_impute_mean(Temperature_Fahrenheit) |>
  step_impute_mean(Non_Invasive_Blood_Pressure_systolic) |>
  step_impute_mean(Non_Invasive_Blood_Pressure_diastolic) |>
  step_impute_mode(marital_status) |>
  step_zv(all_numeric_predictors()) |>
  print()
```

```{r}
# Define parameters to tune in later steps
rf_mod <- 
  rand_forest(
    mode = "classification",
    mtry = tune(),
    min_n = tune(),
    trees = 1000
  ) |> 
  set_engine("ranger", importance = "impurity") |>
  set_mode("classification")
rf_mod
```

```{r}
rf_wf <- workflow() |>
  add_recipe(rf_recipe) |>
  add_model(rf_mod)
```

```{r}
# Exhaustive grid search
rf_param_grid <- grid_regular(
  min_n(range = c(10, 100)),
  mtry(range = c(1L, 5L)),
  levels = c(3, 3)
  )
```


```{r}
set.seed(203)

folds <- vfold_cv(other, v = 3)
```

```{r}
(rf_fit <- rf_wf |>
  tune_grid(
    resamples = folds,
    grid = rf_param_grid,
    metrics = metric_set(roc_auc, accuracy),
    control = control_grid(verbose = TRUE, save_pred = FALSE),
    )) |>
  system.time()
```

```{r}
rf_fit |>
  collect_metrics() |>
  print(width = Inf) |>
  filter(.metric == "roc_auc") |>
  ggplot(mapping = aes(x = min_n, y = mean, color = factor(mtry))) +
  geom_point() + 
  # geom_line() + 
  labs(x = "Min of Sample Split", y = "CV AUC")
```

```{r}
rf_fit |>
  show_best("roc_auc")
```

```{r}
best_rf <- rf_fit |>
  select_best("roc_auc")
```

```{r}
final_wf_rf <- rf_wf |>
  finalize_workflow(best_rf)
final_fit_rf <- 
  final_wf_rf |>
  last_fit(data_split)
```

```{r}
final_fit_rf |> 
  collect_metrics()
```

```{r}
final_fit_rf %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 20)
```

**Summary:** The AUC is 0.6390 and the accuracy is 0.6014 on the test set. The most important features with relative similar importance are 'white blood cells', 'hematocrit', 'non invasive blood pressure systolic', and 'heart rate'.

# XGBoost

```{r}
# Impute the missing values and normalize the data
gb_recipe <- 
  recipe(
    los_long ~ ., 
    data = other
  ) |>
  step_impute_mean(Sodium) |>
  step_impute_mean(Glucose) |>
  step_impute_mean(Chloride) |>
  step_impute_mean(Potassium) |>
  step_impute_mean(Creatinine) |>
  step_impute_mean(Hematocrit) |>
  step_impute_mean(Bicarbonate) |>
  step_impute_mean(White_Blood_Cells) |>
  step_impute_mean(Heart_Rate) |>
  step_impute_mean(Respiratory_Rate) |>
  step_impute_mean(Temperature_Fahrenheit) |>
  step_impute_mean(Non_Invasive_Blood_Pressure_systolic) |>
  step_impute_mean(Non_Invasive_Blood_Pressure_diastolic) |>
  step_impute_mode(marital_status) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_numeric_predictors()) |>
  print()
```

```{r}
# Define parameters to tune in later steps
gb_mod <- 
  boost_tree(
    mode = "classification",
    trees = 1000, 
    tree_depth = tune(),
    learn_rate = tune()
  ) |> 
  set_engine("xgboost")
gb_mod
```

```{r}
gb_wf <- workflow() |>
  add_recipe(gb_recipe) |>
  add_model(gb_mod)
gb_wf
```

```{r}
# Exhaustive grid search
gb_param_grid <- grid_regular(
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-5, 2), trans = log10_trans()),
  levels = c(3, 3)
  )
```


```{r}
set.seed(203)

folds <- vfold_cv(other, v = 3)
```

```{r}
gb_fit <- gb_wf |>
  tune_grid(
    resamples = folds,
    grid = gb_param_grid,
    metrics = metric_set(roc_auc, accuracy),
    control = control_grid(verbose = TRUE, save_pred = FALSE)
    )
gb_fit
```

```{r}
gb_fit |>
  collect_metrics() |>
  print(width = Inf) |>
  filter(.metric == "roc_auc") |>
  ggplot(mapping = aes(x = learn_rate, y = mean, color = factor(tree_depth))) +
  geom_point() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()
```

```{r}
best_gb <- gb_fit |>
  select_best("roc_auc")
```

```{r}
final_wf <- gb_wf |>
  finalize_workflow(best_gb)
```

```{r}
final_fit <- 
  final_wf |>
  last_fit(data_split)
```

```{r}
final_fit |> 
  collect_metrics()
```

```{r}
final_fit |> 
  extract_fit_parsnip() |>
  vip(num_features = 20)
  
```

**Summary:** The AUC is 0.6449 and the accuracy is 0.6035 on the test set. The most important features is "Temperature".

# Model Stacking

```{r}
# Impute the missing values and normalize the data
stack_recipe <- 
  recipe(
    los_long ~ ., 
    data = other
  ) |>
  step_impute_mean(Sodium) |>
  step_impute_mean(Glucose) |>
  step_impute_mean(Chloride) |>
  step_impute_mean(Potassium) |>
  step_impute_mean(Creatinine) |>
  step_impute_mean(Hematocrit) |>
  step_impute_mean(Bicarbonate) |>
  step_impute_mean(White_Blood_Cells) |>
  step_impute_mean(Heart_Rate) |>
  step_impute_mean(Respiratory_Rate) |>
  step_impute_mean(Temperature_Fahrenheit) |>
  step_impute_mean(Non_Invasive_Blood_Pressure_systolic) |>
  step_impute_mean(Non_Invasive_Blood_Pressure_diastolic) |>
  step_impute_mode(marital_status) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_numeric_predictors()) |> 
  step_normalize(all_numeric_predictors()) |>
  print()
```

```{r}
set.seed(203)
folds <- vfold_cv(other, v = 3)
```

```{r}
logit_mod <- 
  logistic_reg(
    penalty = tune(), 
    mixture = tune()
  ) |> 
  set_engine("glmnet", standardize = TRUE)

logit_wf <- workflow() |>
  add_recipe(stack_recipe) |>
  add_model(logit_mod)

logit_grid <- grid_regular(
  penalty(range = c(-6, 3)), 
  mixture(),
  levels = c(5, 5)
  )

logit_res <- 
  tune_grid(
    object = logit_wf, 
    resamples = folds, 
    grid = logit_grid,
    control = control_stack_grid()
  )
```

```{r}
rf_mod <- 
  rand_forest(
    mode = "classification",
    # Number of predictors randomly sampled in each split
    mtry = tune(),
    # Number of trees in ensemble
    trees = tune()
  ) |>
  set_engine("ranger")

rf_wf <- workflow() |>
  add_recipe(stack_recipe) |>
  add_model(rf_mod)

rf_grid <- grid_random(
  trees(range = c(100L, 500L)), 
  mtry(range = c(1L, 5L)),
  size = 2
  )

rf_res <- 
  tune_grid(
    object = rf_wf, 
    resamples = folds, 
    grid = rf_grid,
    control = control_stack_grid()
  )
```

```{r}
gb_mod <- 
  boost_tree(
    mode = "classification",
    trees = 1000, 
    tree_depth = tune(),
    learn_rate = tune()
  ) |> 
  set_engine("xgboost")

gb_wf <- workflow() |>
  add_recipe(stack_recipe) |>
  add_model(gb_mod)

gb_grid <- grid_random(
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-5, 2), trans = log10_trans()),
  size = 4
  )

gb_res <-
  tune_grid(
    object = gb_wf, 
    resamples = folds, 
    grid = gb_grid,
    control = control_stack_grid()
    )
```

```{r}
(model_st <- 
  # initialize the stack
  stacks() |>
  # add candidate members
  add_candidates(logit_res) |>
  add_candidates(rf_res) |>
  add_candidates(gb_res) |>
  # determine how to combine their predictions
  blend_predictions(
    penalty = 10^(-6:2),
    metrics = c("roc_auc")
    ) |>
  # fit the candidates with nonzero stacking coefficients
  fit_members()) |>
  system.time()
```

```{r}
model_st
```

```{r}
autoplot(model_st)
```

```{r}
autoplot(model_st, type = "members")
```

```{r}
autoplot(model_st, type = "weights")
```

```{r}
collect_parameters(model_st, "rf_res")
```

```{r}
pred <- test %>%
  bind_cols(predict(model_st, ., type = "prob")) %>%
  print(width = Inf)
```

```{r}
yardstick::roc_auc(
  pred,
  truth = los_long,
  contains(".pred_0")
  )
```


4. Compare model classification performance on the test set. Report both the area under ROC curve and accuracy for each machine learning algorithm and the model stacking. Interpret the results. What are the most important features in predicting long ICU stays? How do the models compare in terms of performance and interpretability?

**Answer:**

The logistic regression performs the worst in this task. It has the lowest AUC and accuracy. The random forest and gradient boosting models perform similarly, with the random forest model having a slightly higher AUC and accuracy. The model stacking has the highest AUC and accuracy. 

The three independent models have different ranking for feature importance. Here is the report of the top three variables selected in each model. Logistic regression selects 'heart rate', and 'first care unit', and 'hematocrit' as most important features. Random Forest selects 'white Blood cells', 'heart rate', 'non invasive blood pressure systolic' as most important features. XGBoost selects 'heart rate', 'non invasive blood pressure systolic', and 'temperature fahrenheit' as most important features. Among those features, Both three models agree that 'heart rate' is very important feature. Random Forest and XGBoost also agree that 'non invasive blood pressure systolic' is a important feature in predicting long ICU stays. The difference of feature importance rank between logistic regression and the other two models are reasonable since logistic regression is a linear model which may not capture the non-linear relationship between the features and the target variable. 





